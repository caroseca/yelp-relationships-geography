{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4842f8e7-7aa5-4878-b31e-eb3fe298c203",
   "metadata": {},
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42261544-9cf9-459b-9c7a-2e7ad7929c59",
   "metadata": {},
   "source": [
    "## 2.0. Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb745d3b-688a-40db-9e14-b93ca2162d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages to use later\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c151a2-e67f-45f4-8bed-de4ad750a63d",
   "metadata": {},
   "source": [
    "## 2.1. Load & Process Relationship Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496706e8-2533-4890-a420-c7756a0a5e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['child,children,kid,kids', 'daughter,daughters', 'son,sons', 'parent,parents', 'mother,mom', 'father,dad', 'brother,brothers', 'sister,sisters', 'siblings', 'aunt,aunts', 'uncle,uncles', 'niece,nieces', 'nephew,nephews', 'cousin,cousins', 'grandchild,grandchildren', 'grandmother,grandma', 'grandfather,grandpa', 'grandparents', 'spouse', 'partner', 'husband', 'wife', 'bff', 'relationship', 'date', 'boo,bae,sweetheart', 'fiancee,fiance', 'girlfriend,gf', 'boyfriend,bf', 'friend,friends,buddy,buddies,pal,pals', 'housemate,housemates,roommate,roommates,flatmate,flatmates', 'neighbor,neighbors', 'classmate,classmates', 'professor,professors', 'teacher,teachers', 'coworker,coworkers,colleague,colleagues', 'client,clients', 'boss']\n",
      "{'my_aunt', 'our_daughter', 'my_son', 'my_father', 'my_bf', 'my_parents', 'my_cousins', 'my_parent', 'our_sons', 'my_buddies', 'my_fiance', 'my_brothers', 'my_grandpa', 'my_sister', 'my_classmates', 'our_daughters', 'my_client', 'our_child', 'my_pals', 'my_brother', 'my_gf', 'my_children', 'my_housemates', 'my_kid', 'my_teachers', 'my_coworkers', 'our_kids', 'my_grandma', 'my_housemate', 'my_sisters', 'my_nephew', 'my_relationship', 'my_professors', 'my_sons', 'my_buddy', 'our_kid', 'my_teacher', 'our_children', 'my_roommate', 'my_girlfriend', 'my_uncles', 'my_kids', 'my_siblings', 'my_bae', 'my_fiancee', 'my_uncle', 'my_flatmates', 'my_professor', 'my_aunts', 'my_child', 'my_mom', 'my_grandmother', 'my_husband', 'my_wife', 'my_classmate', 'my_boss', 'my_colleagues', 'my_boo', 'my_sweetheart', 'my_colleague', 'my_grandchildren', 'my_grandfather', 'my_grandchild', 'my_nieces', 'my_neighbors', 'my_daughters', 'my_date', 'my_dad', 'my_friends', 'my_daughter', 'my_partner', 'my_mother', 'my_bff', 'my_niece', 'my_coworker', 'my_pal', 'my_neighbor', 'my_grandparents', 'my_spouse', 'my_roommates', 'my_friend', 'my_boyfriend', 'our_son', 'my_clients', 'my_flatmate', 'my_nephews', 'my_cousin'}\n"
     ]
    }
   ],
   "source": [
    "# Why should we set up the relationships data like this?\n",
    "# 1) Prevents need for stemming each word in each review (time-consuming)\n",
    "# 2) Allows arbitrary groupings of relationships beyond stemming equivalence\n",
    "#    (e.g. \"roommate\" and \"housemate\", \"children\" and \"kids\", etc.)\n",
    "\n",
    "with open(r\"relationships/Relationships_ATUS_custom_v2.txt\") as f:\n",
    "    relationships = [line.strip().lower() for line in f.readlines()]\n",
    "\n",
    "relationships_dict = dict()  ## From relationship category to all relevant relationship words, e.g. \"spouse\" --> [\"spouse\", \"partner\"]\n",
    "relationships_dict_reverse = dict()  ## From any relationship word to its category, e.g. \"partner\" --> \"spouse\"\n",
    "\n",
    "for line in relationships:\n",
    "    relevant_words = line.split(\",\")\n",
    "    category = relevant_words[0]\n",
    "    relationships_dict[category] = relevant_words.copy()\n",
    "    for word in relevant_words:\n",
    "        relationships_dict_reverse[word] = category\n",
    "\n",
    "full_relationship_set = set()\n",
    "for relationship_list in relationships_dict.values():\n",
    "    full_relationship_set.update(relationship_list)\n",
    "        \n",
    "print(relationships)\n",
    "#print()\n",
    "#print(relationships_dict)\n",
    "#print()\n",
    "#print(relationships_dict_reverse)\n",
    "\n",
    "words_need_our = [\"child\", \"children\", \"kid\", \"kids\", \"son\", \"sons\", \"daughter\", \"daughters\"]\n",
    "\n",
    "full_relationship_set_new = set()\n",
    "for word in full_relationship_set:\n",
    "    full_relationship_set_new.add(\"my_\" + word)\n",
    "    if word in words_need_our:\n",
    "        full_relationship_set_new.add(\"our_\" + word)\n",
    "print(full_relationship_set_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f0557-bdf0-4157-a114-103d81e54f29",
   "metadata": {},
   "source": [
    "## 2.2. Text Mining\n",
    "### Note that for the purposes of this exercise, we're just going to use the Boulder data set for text mining and processing counts. No need to iterate through the metro areas, so we're going to explicitly read in the Boulder csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ccfe00f-583f-4c30-8f76-a4cae42e8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metros = ['Boston', 'Portland', 'Austin', 'Orlando', 'Atlanta', 'Vancouver', 'Columbus', 'Boulder']\n",
    "\n",
    "# metro = 'Vancouver'\n",
    "# metro_reviews = pd.read_csv(\"small_reviews/yelp_academic_dataset_reviews_\" + metro + \".csv\")\n",
    "# metro_reviews = pd.read_csv(\"small_reviews_urbcomp/yelp_academic_dataset_reviews_\" + metro + \".csv\")\n",
    "#metro_reviews.head(3)\n",
    "metro_reviews = pd.read_csv(r\"small_reviews_urbcomp/yelp_academic_dataset_reviews_Boulder.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "354d2a4b-55f2-4ef2-a409-a7ae9f62017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.882421255111694 sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Clean review text by making everything lowercase\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text\"].str.lower()\n",
    "\n",
    "#note that the words that cue a relationship are ---my--- & ---our---\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text_clean\"].str.replace(\"my \", \"my_\")\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text_clean\"].str.replace(\"our \", \"our_\")\n",
    "\n",
    "#print(metro_reviews[metro_reviews[\"review_id\"] == \"k9vlSSUStwY2DcjM8Rinnw\"].iloc[0, -1])\n",
    "\n",
    "# Apply tokenizer and get rid of punctuation\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text_clean\"].fillna(\"0\")\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text_clean\"].apply(tokenizer.tokenize)\n",
    "\n",
    "# Remove duplicate words in each review\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text_clean\"].apply(set)\n",
    "\n",
    "# Join tokens with spaces into strings for easy word counting\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text_clean\"].apply(\" \".join)\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "721ad0a3-f7eb-467b-8321-aab814100df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>funny</th>\n",
       "      <th>useful</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>datetime</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sjm_uUcQVxab_EeLCqsYLg</td>\n",
       "      <td>The food is always great here. The service fro...</td>\n",
       "      <td>8zehGz9jnxPqXtOc7KaJxA</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2011-07-28 18:05:01</td>\n",
       "      <td>0kA0PAJ8QFMeveQWHFqz2A</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-07-28 18:05:01</td>\n",
       "      <td>if only can of from you is service back patio ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eiAeuhR3kurAO8rAt_rhlg</td>\n",
       "      <td>Brasserie zero zero... we were hoping for a te...</td>\n",
       "      <td>8zehGz9jnxPqXtOc7KaJxA</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-08-04 20:52:32</td>\n",
       "      <td>7zEJt0NVl-lMiMwkCsvteg</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-08-04 20:52:32</td>\n",
       "      <td>of disappointing even size mustard tough start...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tanL1f9UuVPE2veDDHAL-w</td>\n",
       "      <td>Excellent subs at a reasonable price. A huge v...</td>\n",
       "      <td>i9BDFBYcl_PGqrLbQUdMvg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2016-06-22 05:13:56</td>\n",
       "      <td>h_kjYBpubAmYVxvNaID7Lw</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-22 05:13:56</td>\n",
       "      <td>can decide of ranch those and reasonable huge ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  funny  useful               review_id  \\\n",
       "0           4      0       0  sjm_uUcQVxab_EeLCqsYLg   \n",
       "1          53      0       0  eiAeuhR3kurAO8rAt_rhlg   \n",
       "2         185      0       0  tanL1f9UuVPE2veDDHAL-w   \n",
       "\n",
       "                                                text             business_id  \\\n",
       "0  The food is always great here. The service fro...  8zehGz9jnxPqXtOc7KaJxA   \n",
       "1  Brasserie zero zero... we were hoping for a te...  8zehGz9jnxPqXtOc7KaJxA   \n",
       "2  Excellent subs at a reasonable price. A huge v...  i9BDFBYcl_PGqrLbQUdMvg   \n",
       "\n",
       "   stars                 date                 user_id  cool  \\\n",
       "0    4.0  2011-07-28 18:05:01  0kA0PAJ8QFMeveQWHFqz2A     0   \n",
       "1    2.0  2018-08-04 20:52:32  7zEJt0NVl-lMiMwkCsvteg     0   \n",
       "2    4.0  2016-06-22 05:13:56  h_kjYBpubAmYVxvNaID7Lw     0   \n",
       "\n",
       "              datetime                                         text_clean  \n",
       "0  2011-07-28 18:05:01  if only can of from you is service back patio ...  \n",
       "1  2018-08-04 20:52:32  of disappointing even size mustard tough start...  \n",
       "2  2016-06-22 05:13:56  can decide of ranch those and reasonable huge ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metro_reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9840c6d1-081b-4fb3-8c56-525259962066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.887014627456665 sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "relationship_categories = sorted(relationships_dict.keys())\n",
    "relationship_categories_ungroup = sorted(relationships_dict_reverse.keys())\n",
    "\n",
    "df_rows = []\n",
    "df_rows_ungroup = []\n",
    "\n",
    "for i, business_id in enumerate(metro_reviews.business_id.unique()):\n",
    "    reviews_subset = metro_reviews[metro_reviews[\"business_id\"] == business_id]\n",
    "    reviews_subset_counts = reviews_subset.text_clean.str.split().explode().value_counts().reset_index()\n",
    "    x = reviews_subset_counts[reviews_subset_counts[\"index\"].isin(full_relationship_set_new)]\n",
    "#     print(x)\n",
    "#     break\n",
    "    df_row = [business_id, len(reviews_subset)] + [0 for key in relationship_categories]\n",
    "    df_row_ungroup = [business_id, len(reviews_subset)] + [0 for key in relationship_categories_ungroup]\n",
    "\n",
    "    for row in x.itertuples():\n",
    "        key = row.index.split(\"_\")[-1]\n",
    "        \n",
    "        df_row_idx = 2 + relationship_categories.index(relationships_dict_reverse[key])\n",
    "        df_row[df_row_idx] += row.text_clean\n",
    "        \n",
    "        df_row_idx_ungroup = 2 + relationship_categories_ungroup.index(key)\n",
    "        df_row_ungroup[df_row_idx_ungroup] += row.text_clean\n",
    "        \n",
    "    df_rows.append(df_row)\n",
    "    df_rows_ungroup.append(df_row_ungroup)\n",
    "\n",
    "relationship_df = pd.DataFrame(df_rows,columns = [\"business_id\", \"num_reviews\"] + relationship_categories)\n",
    "relationship_df['num_relationship_words'] = relationship_df[relationship_categories].sum(axis=1)\n",
    "relationship_df = relationship_df[[\"business_id\", \"num_reviews\", \"num_relationship_words\"] + relationship_categories]\n",
    "\n",
    "relationship_df_ungroup = pd.DataFrame(df_rows_ungroup,columns = [\"business_id\", \"num_reviews\"] + relationship_categories_ungroup)\n",
    "relationship_df_ungroup['num_relationship_words'] = relationship_df_ungroup[relationship_categories_ungroup].sum(axis=1)\n",
    "relationship_df_ungroup = relationship_df_ungroup[[\"business_id\", \"num_reviews\", \"num_relationship_words\"] + relationship_categories_ungroup]\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d8c5d18-7e8b-4fd9-b640-0f27cf4916fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_df.to_csv(r\"output_urbcomp/699counts_v2.csv\", index=False)\n",
    "relationship_df_ungroup.to_csv(r\"output_urbcomp/699counts_ungrouped_v2.csv\", index=False)\n",
    "# relationship_df.to_csv(\"output/\" + metro + \"_counts_v2.csv\", index=False)\n",
    "# relationship_df_ungroup.to_csv(\"output/\" + metro + \"_counts_ungrouped_v2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
